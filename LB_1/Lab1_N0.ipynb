{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext, SparkConf\n",
    "import os\n",
    "import pyspark.sql as sql\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import DoubleType\n",
    "from pyspark.sql.functions import udf, col, max, sum, countDistinct"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Инициализация сессии"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkRuntimeError",
     "evalue": "[JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32mf:\\Homework(magistr)\\BigData\\Practies\\Lab_1\\Lab1_N0.ipynb Cell 3\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Homework%28magistr%29/BigData/Practies/Lab_1/Lab1_N0.ipynb#W2sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark \u001b[39m=\u001b[39m SparkSession\u001b[39m.\u001b[39;49mbuilder\u001b[39m.\u001b[39;49mappName(\u001b[39m\"\u001b[39;49m\u001b[39mLab1_Interactive_bike_analysis\u001b[39;49m\u001b[39m\"\u001b[39;49m)\u001b[39m.\u001b[39;49mgetOrCreate()\n",
      "File \u001b[1;32mc:\\Users\\Vasser232\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\sql\\session.py:497\u001b[0m, in \u001b[0;36mSparkSession.Builder.getOrCreate\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    495\u001b[0m     sparkConf\u001b[39m.\u001b[39mset(key, value)\n\u001b[0;32m    496\u001b[0m \u001b[39m# This SparkContext may be an existing one.\u001b[39;00m\n\u001b[1;32m--> 497\u001b[0m sc \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39;49mgetOrCreate(sparkConf)\n\u001b[0;32m    498\u001b[0m \u001b[39m# Do not update `SparkConf` for existing `SparkContext`, as it's shared\u001b[39;00m\n\u001b[0;32m    499\u001b[0m \u001b[39m# by all sessions.\u001b[39;00m\n\u001b[0;32m    500\u001b[0m session \u001b[39m=\u001b[39m SparkSession(sc, options\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_options)\n",
      "File \u001b[1;32mc:\\Users\\Vasser232\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:515\u001b[0m, in \u001b[0;36mSparkContext.getOrCreate\u001b[1;34m(cls, conf)\u001b[0m\n\u001b[0;32m    513\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    514\u001b[0m     \u001b[39mif\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m--> 515\u001b[0m         SparkContext(conf\u001b[39m=\u001b[39;49mconf \u001b[39mor\u001b[39;49;00m SparkConf())\n\u001b[0;32m    516\u001b[0m     \u001b[39massert\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[0;32m    517\u001b[0m     \u001b[39mreturn\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_active_spark_context\n",
      "File \u001b[1;32mc:\\Users\\Vasser232\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:201\u001b[0m, in \u001b[0;36mSparkContext.__init__\u001b[1;34m(self, master, appName, sparkHome, pyFiles, environment, batchSize, serializer, conf, gateway, jsc, profiler_cls, udf_profiler_cls, memory_profiler_cls)\u001b[0m\n\u001b[0;32m    195\u001b[0m \u001b[39mif\u001b[39;00m gateway \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m gateway\u001b[39m.\u001b[39mgateway_parameters\u001b[39m.\u001b[39mauth_token \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m    196\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m    197\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mYou are trying to pass an insecure Py4j gateway to Spark. This\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    198\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m is not allowed as it is a security risk.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m    199\u001b[0m     )\n\u001b[1;32m--> 201\u001b[0m SparkContext\u001b[39m.\u001b[39;49m_ensure_initialized(\u001b[39mself\u001b[39;49m, gateway\u001b[39m=\u001b[39;49mgateway, conf\u001b[39m=\u001b[39;49mconf)\n\u001b[0;32m    202\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m    203\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_do_init(\n\u001b[0;32m    204\u001b[0m         master,\n\u001b[0;32m    205\u001b[0m         appName,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    215\u001b[0m         memory_profiler_cls,\n\u001b[0;32m    216\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Vasser232\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\context.py:436\u001b[0m, in \u001b[0;36mSparkContext._ensure_initialized\u001b[1;34m(cls, instance, gateway, conf)\u001b[0m\n\u001b[0;32m    434\u001b[0m \u001b[39mwith\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_lock:\n\u001b[0;32m    435\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m SparkContext\u001b[39m.\u001b[39m_gateway:\n\u001b[1;32m--> 436\u001b[0m         SparkContext\u001b[39m.\u001b[39m_gateway \u001b[39m=\u001b[39m gateway \u001b[39mor\u001b[39;00m launch_gateway(conf)\n\u001b[0;32m    437\u001b[0m         SparkContext\u001b[39m.\u001b[39m_jvm \u001b[39m=\u001b[39m SparkContext\u001b[39m.\u001b[39m_gateway\u001b[39m.\u001b[39mjvm\n\u001b[0;32m    439\u001b[0m     \u001b[39mif\u001b[39;00m instance:\n",
      "File \u001b[1;32mc:\\Users\\Vasser232\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\pyspark\\java_gateway.py:107\u001b[0m, in \u001b[0;36mlaunch_gateway\u001b[1;34m(conf, popen_kwargs)\u001b[0m\n\u001b[0;32m    104\u001b[0m     time\u001b[39m.\u001b[39msleep(\u001b[39m0.1\u001b[39m)\n\u001b[0;32m    106\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m os\u001b[39m.\u001b[39mpath\u001b[39m.\u001b[39misfile(conn_info_file):\n\u001b[1;32m--> 107\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkRuntimeError(\n\u001b[0;32m    108\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mJAVA_GATEWAY_EXITED\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m    109\u001b[0m         message_parameters\u001b[39m=\u001b[39m{},\n\u001b[0;32m    110\u001b[0m     )\n\u001b[0;32m    112\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mopen\u001b[39m(conn_info_file, \u001b[39m\"\u001b[39m\u001b[39mrb\u001b[39m\u001b[39m\"\u001b[39m) \u001b[39mas\u001b[39;00m info:\n\u001b[0;32m    113\u001b[0m     gateway_port \u001b[39m=\u001b[39m read_int(info)\n",
      "\u001b[1;31mPySparkRuntimeError\u001b[0m: [JAVA_GATEWAY_EXITED] Java gateway process exited before sending its port number."
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"Lab1_Interactive_bike_analysis\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'spark' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mf:\\Homework(magistr)\\BigData\\Practies\\Lab_1\\Lab1_N0.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m----> <a href='vscode-notebook-cell:/f%3A/Homework%28magistr%29/BigData/Practies/Lab_1/Lab1_N0.ipynb#W3sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m spark\u001b[39m.\u001b[39mversion\n",
      "\u001b[1;31mNameError\u001b[0m: name 'spark' is not defined"
     ]
    }
   ],
   "source": [
    "spark.version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Загрузка данных"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = os.path.join(os.curdir, \"data\")\n",
    "trips_path = os.path.join(data_path, \"trips.csv\")\n",
    "stations_path = os.path.join(data_path, \"stations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trips\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- duration: integer (nullable = true)\n",
      " |-- start_date: timestamp (nullable = true)\n",
      " |-- start_station_name: string (nullable = true)\n",
      " |-- start_station_id: integer (nullable = true)\n",
      " |-- end_date: timestamp (nullable = true)\n",
      " |-- end_station_name: string (nullable = true)\n",
      " |-- end_station_id: integer (nullable = true)\n",
      " |-- bike_id: integer (nullable = true)\n",
      " |-- subscription_type: string (nullable = true)\n",
      " |-- zip_code: string (nullable = true)\n",
      "\n",
      "Stations\n",
      "root\n",
      " |-- id: integer (nullable = true)\n",
      " |-- name: string (nullable = true)\n",
      " |-- lat: double (nullable = true)\n",
      " |-- long: double (nullable = true)\n",
      " |-- dock_count: integer (nullable = true)\n",
      " |-- city: string (nullable = true)\n",
      " |-- installation_date: string (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "trip_data = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"timestampFormat\", \"M/d/y H:m\")\n",
    "    .csv(trips_path)\n",
    ")\n",
    "\n",
    "stations_data = (\n",
    "    spark.read.option(\"header\", True)\n",
    "    .option(\"inferSchema\", True)\n",
    "    .option(\"timestampFormat\", \"M/d/y H:m\")\n",
    "    .csv(stations_path)\n",
    ")\n",
    "\n",
    "print(\"Trips\")\n",
    "trip_data.printSchema()\n",
    "print(\"Stations\")\n",
    "stations_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 1. Найти велосипед с максимальным временем пробега"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Велосипед: 535 с максимальным пробегом: 18611693\n"
     ]
    }
   ],
   "source": [
    "# Группировка по id велосипеда и подсчет вермени пробега каждого велосипеда\n",
    "max_trip_bike = trip_data.groupBy(\"bike_id\").agg(\n",
    "    sum(col(\"duration\")).alias(\"total_trips_duration\")\n",
    ")\n",
    "\n",
    "\n",
    "# Выбор велосипеда с максимальным пробегом\n",
    "bike_with_max_duration = max_trip_bike.orderBy(\n",
    "    col(\"total_trips_duration\").desc()\n",
    ").first()\n",
    "\n",
    "# bike_id\n",
    "bike_id = bike_with_max_duration[\"bike_id\"]\n",
    "\n",
    "# Значение пробега\n",
    "duration = bike_with_max_duration[\"total_trips_duration\"]\n",
    "\n",
    "print(f\"Велосипед: {bike_id} с максимальным пробегом: {duration}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задание 2. Найти наибольшее геодезическое расстояние между станциями"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum geodesic distance between stations: 0.7058482821754397\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import DoubleType\n",
    "\n",
    "\n",
    "# Функция для вычисления расстояния между станциями\n",
    "def distance(lat1, long1, lat2, long2):\n",
    "    return ((lat1 - lat2) ** 2 + (long1 - long2) ** 2) ** 0.5\n",
    "\n",
    "\n",
    "# Конвертация функции в pyspark.sql.functions.udf\n",
    "distance_udf = udf(distance, DoubleType())\n",
    "\n",
    "# Создание различных комбинаций станций\n",
    "station_combinations = stations_data.alias(\"station1\").crossJoin(\n",
    "    stations_data.alias(\"station2\")\n",
    ")\n",
    "\n",
    "# Вычилсение расстояния для каждой пары станции\n",
    "joined_stations = station_combinations.withColumn(\n",
    "    \"distance\",\n",
    "    distance_udf(\n",
    "        col(\"station1.lat\"),\n",
    "        col(\"station1.long\"),\n",
    "        col(\"station2.lat\"),\n",
    "        col(\"station2.long\"),\n",
    "    ),\n",
    ")\n",
    "\n",
    "# Поиск максимального расстояния\n",
    "max_distance = joined_stations.agg({\"distance\": \"max\"}).collect()[0][0]\n",
    "\n",
    "print(f\"Maximum geodesic distance between stations: {max_distance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Найти путь велосипеда с максимальным временем пробега через станции"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Максимальное время пробега 17270400 секунд \n",
      " Из станции South Van Ness at Market на станцию 2nd at Folsom\n"
     ]
    }
   ],
   "source": [
    "# Сортировка по столбцу duration и выбор наиболее длительной поездки\n",
    "trip_max_distance = (\n",
    "    trip_data.select(\"start_station_name\", \"end_station_name\", \"duration\")\n",
    "    .orderBy(col(\"duration\").desc())\n",
    "    .first()\n",
    ")\n",
    "\n",
    "# Начальная, конечная станция, а также время поездки\n",
    "start_location = trip_max_distance[\"start_station_name\"]\n",
    "end_location = trip_max_distance[\"end_station_name\"]\n",
    "trip_time = trip_max_distance[\"duration\"]\n",
    "\n",
    "print(\n",
    "    f\"Максимальное время пробега {trip_time} секунд \\n Из станции {start_location} на станцию {end_location}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Найти количество велосипедов в системе"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Количество велосипедов в системе: 700\n"
     ]
    }
   ],
   "source": [
    "bike_counts = trip_data.agg(countDistinct(\"bike_id\").alias(\"bike_count\")).collect()[0][\"bike_count\"]\n",
    "\n",
    "print(f\"Количество велосипедов в системе: {bike_counts}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Найти пользователей потративших на поездки более 3 часов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+----------+\n",
      "|bike_id|total_time|\n",
      "+-------+----------+\n",
      "|    471|   1718831|\n",
      "|    496|   1679568|\n",
      "|    148|    332138|\n",
      "|    463|   1722796|\n",
      "|    540|   1752835|\n",
      "|    392|   1789476|\n",
      "|    623|   2037219|\n",
      "|    516|   1896751|\n",
      "|     31|    407907|\n",
      "|    580|   1034382|\n",
      "|    137|   1529200|\n",
      "|    251|   1282980|\n",
      "|    451|   1695574|\n",
      "|    458|   1647080|\n",
      "|     65|    216922|\n",
      "|    588|    266415|\n",
      "|    255|    396395|\n",
      "|     53|    226389|\n",
      "|    481|   1925143|\n",
      "|    472|   1620686|\n",
      "+-------+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Группировка по id велосипеда и подсчет общего времени, проведенного в поездке\n",
    "users_with_total_trip_time = trip_data.groupBy(\"bike_id\").sum(\"duration\").withColumnRenamed(\"sum(duration)\", \"total_time\")\n",
    "users_with_total_trip_time.filter(\"total_time>10800\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
